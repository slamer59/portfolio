---
title: "Edge Computing ML: Docker for Jetson Xavier & ARM64"
slug: "edge-computing-docker-jetson-xavier"
date: "2025-04-25T10:00:00.000Z"
summary: "Deploy ML models on edge devices with optimized Docker containers. TensorRT optimization for NVIDIA Jetson Xavier achieving 78ms latency and 1.8GB images."
author: "Thomas Pedot"
technologies: ["Docker", "TensorRT", "NVIDIA Jetson", "ARM64", "Machine Learning"]
image: "/images/devprojects/edge-computing-jetson.png"
published: true
featured: true
keywords: ["edge computing", "docker arm64", "machine learning deployment", "tensorrt optimization", "nvidia jetson"]
type: "article"
github: "https://github.com/slamer59/jetson-containers"
pillarPage: "cloud-native-devops-expertise"
pillarTitle: "Kubernetes Production & DevOps: Helm, GitOps, ArgoCD"
---

> üìö Part of the [Cloud Native & DevOps Expertise](/articles/cloud-native-devops-expertise) series

## Edge Computing : L'Informatique √† la P√©riph√©rie

L'edge computing transforme la fa√ßon dont nous pensons le traitement des donn√©es : **traiter o√π les donn√©es sont g√©n√©r√©es, pas dans le cloud**.

### D√©finition

**Edge Computing** signifie :
- Ex√©cution de calculs sur le dispositif local
- Latence minimale
- Fonctionnement hors ligne
- Pr√©servation de la confidentialit√©

## Le D√©fi Technique

### Contraintes de l'Edge Computing

Sur un NVIDIA Jetson Xavier (ARM64), nos d√©fis :

- **Latence :** < 100ms
- **RAM :** Limit√©e √† 32GB
- **Puissance :** < 20W
- **Stockage :** Images Docker < 2GB

### Architecture Sp√©cifique

**NVIDIA Jetson Xavier** ‚â† Serveur classique x86_64

Diff√©rences critiques :
1. Architecture ARM64 vs x86_64
2. Drivers CUDA sp√©cifiques
3. GPU embarqu√© limit√© (512 CUDA cores)
4. Optimisations hardware uniques

## Solution : Conteneurs Docker Optimis√©s

### Multi-Stage Build

```dockerfile
# Stage 1: Builder (compilation des d√©pendances)
FROM nvcr.io/nvidia/l4t-ml:r35.2.1-py3 AS builder

# Installer les outils de build
RUN apt-get update && apt-get install -y \
    build-essential cmake git

# Compiler TensorFlow from source avec optimisations ARM64
RUN git clone https://github.com/tensorflow/tensorflow.git && \
    cd tensorflow && \
    bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

# Stage 2: Runtime (image minimale)
FROM nvcr.io/nvidia/l4t-base:r35.2.1

# Copier uniquement les binaires compil√©s
COPY --from=builder /tensorflow/bazel-bin /opt/tensorflow

# Installer les d√©pendances runtime minimales
RUN apt-get update && apt-get install -y --no-install-recommends \
    libcudnn8 libnvinfer8 && \
    rm -rf /var/lib/apt/lists/*

# R√©sultat : 1.8GB vs 8GB original
```

### Optimisation TensorRT

```python
# optimize_model.py
import tensorflow as tf
from tensorflow.python.compiler.tensorrt import trt_convert as trt

def optimize_for_jetson(model_path: str, output_path: str):
    """Optimiser un mod√®le TensorFlow avec TensorRT pour Jetson."""

    # Charger le mod√®le
    model = tf.saved_model.load(model_path)

    # Convertir en TensorRT
    converter = trt.TrtGraphConverterV2(
        input_saved_model_dir=model_path,
        precision_mode=trt.TrtPrecisionMode.FP16,  # Pr√©cision moiti√© pour vitesse
        maximum_cached_engines=100
    )
    converter.convert()

    # Sauvegarder le mod√®le optimis√©
    converter.save(output_path)

    print(f"‚úÖ Mod√®le optimis√©: {model_path} ‚Üí {output_path}")
    print(f"   Pr√©cision: FP16 (2x plus rapide que FP32)")
```

### Pipeline d'Inf√©rence Temps R√©el

```python
# inference.py
import cv2
import tensorrt as trt
import numpy as np

class JetsonInference:
    def __init__(self, model_path: str):
        self.engine = self.load_engine(model_path)
        self.context = self.engine.create_execution_context()

    def infer(self, image: np.ndarray) -> List[Detection]:
        """Ex√©cuter l'inf√©rence sur une image."""
        start = time.time()

        # Pr√©traitement
        input_tensor = self.preprocess(image)

        # Inf√©rence (GPU)
        outputs = self.context.execute_v2(bindings=[input_tensor])

        # Post-traitement
        detections = self.postprocess(outputs)

        latence = (time.time() - start) * 1000
        print(f"Inf√©rence: {latence:.1f}ms")

        return detections
```

## R√©sultats Mesurables

### YOLOv5 D√©tection d'Objets

**Configuration :**
- Input: 1080p @ 30fps
- Latence : **78ms par image** (< 100ms cible ‚úÖ)
- Pr√©cision : mAP 0.89
- Puissance : **14W** (< 20W cible ‚úÖ)
- Image Docker : **1.8GB** (< 2GB cible ‚úÖ)

### Comparaison Cloud vs Edge

| M√©trique | Cloud GPU (A100) | Jetson Xavier Edge |
|----------|------------------|---------------------|
| Latence | 12ms + 80ms r√©seau = **92ms** | **78ms** (pas de r√©seau) ‚úÖ |
| Co√ªt | 2.50$/heure | 500$ unique |
| Hors ligne | ‚ùå | ‚úÖ |
| Confidentialit√© | ‚ùå (donn√©es envoy√©es au cloud) | ‚úÖ (traitement local) |

## Related Cloud Native Articles

- [Helm Chart Visualization Tool](/articles/helm-chart-visualization-tool) - Ma√Ætriser les d√©pendances Helm
- [ArgoCD + Dagster: Multi-Tenant GitOps](/articles/argocd-dagster-gitops-production) - Architecture de d√©ploiement automatis√©e

## Le√ßons Apprises

1. **ARM64 ‚â† x86_64** : Recompiler depuis les sources
2. **TensorRT est magique** : Acc√©l√©ration 2-3x en pr√©cision FP16
3. **Ordre des layers Docker** : Critique pour les temps de build
4. **Contraintes = Innovation** : Limitations forcent l'optimisation

## Conclusion

L'edge computing n'est pas qu'une alternative au cloud. C'est une approche qui replace le calcul l√† o√π il a le plus de sens : **au plus pr√®s des donn√©es**.

<ViewRepository url="https://github.com/slamer59/jetson-containers" text="Voir le projet sur GitHub" />

---

**Explorez tous mes projets Cloud Native ‚Üí** [Cloud Native & DevOps Hub](/articles/cloud-native-devops-expertise)
